{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FKIDyx5NARF",
        "outputId": "0c9849a4-25df-4ecc-8a20-ba003e013577"
      },
      "outputs": [],
      "source": [
        "# Dataset to train the model with numbers in sign language\n",
        "# It's a Kaggle dataset available at the following link:\n",
        "# https://www.kaggle.com/datasets/ardamavi/sign-language-digits-dataset\n",
        "# It's uploaded to a git repository, so we'll clone it directly\n",
        "!git clone https://github.com/ardamavi/Sign-Language-Digits-Dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A87rQqMO-g2k",
        "outputId": "5e096766-4303-412e-9d09-af9d1e1e3e8b"
      },
      "outputs": [],
      "source": [
        "# We import all the necessary packages.\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models                import Sequential, Model\n",
        "from tensorflow.keras.layers                import Flatten, Dropout, Dense\n",
        "from tensorflow.keras.preprocessing.image   import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers            import SGD\n",
        "from tensorflow.keras                       import regularizers\n",
        "\n",
        "# Callbacks\n",
        "from tensorflow.keras.callbacks             import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks             import ReduceLROnPlateau\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.image   import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "8K7gJuvdYnJi"
      },
      "outputs": [],
      "source": [
        "# Variables for the model operation\n",
        "clases = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "num_classes = len(clases) # There are a total of 10 classes.\n",
        "epochs = 50\n",
        "bs = 32 # Bachsize\n",
        "k = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6I75lgKTNvF",
        "outputId": "8e3fe369-684a-45a0-a1ea-64f54c400f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1653 images belonging to 10 classes.\n",
            "Found 409 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# We create image generators for the train and test/val datasets\n",
        "# ImageDataGenerator is a Keras class that allows data augmentation\n",
        "# for image data in real-time during training.\n",
        "\n",
        "# Training image generator.\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=(0.3),\n",
        "        zoom_range=(0.3),\n",
        "        width_shift_range=(0.2),\n",
        "        height_shift_range=(0.2),\n",
        "        validation_split = 0.2,\n",
        "        brightness_range=(0.05,0.85),\n",
        "        horizontal_flip=False)\n",
        "\n",
        "# Loading images into the training generator from directory.\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/Sign-Language-Digits-Dataset/Dataset',\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        target_size=(28*k, 28*k),\n",
        "        color_mode = 'rgb',\n",
        "        subset = 'training',\n",
        "        batch_size=bs)\n",
        "\n",
        "valid_generator = train_datagen.flow_from_directory(\n",
        "        '/content/Sign-Language-Digits-Dataset/Dataset',\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        target_size=(28*k, 28*k),\n",
        "        color_mode = 'rgb',\n",
        "        subset = 'validation',\n",
        "        batch_size=bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLjOXlKbTOx3",
        "outputId": "2fa77de0-f25d-40c5-b41d-d374912925c3"
      },
      "outputs": [],
      "source": [
        "# We import the model we are going to use, which is VGG19.\n",
        "VGG19_model = tf.keras.applications.VGG19(input_shape=(28*k,28*k,3),\n",
        "                                          include_top=False,\n",
        "                                          weights='imagenet')\n",
        "\n",
        "# We display the layers of this model.\n",
        "VGG19_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY4ZlW7uTSH8"
      },
      "outputs": [],
      "source": [
        "# Freezing the first layers of a convolutional model, such as VGG19, is because\n",
        "# they generally learn basic and generic features, such as edges,\n",
        "# simple textures, etc. These features are useful for a wide range of\n",
        "# computer vision tasks and are often transferable to different datasets\n",
        "# and problems.\n",
        "\n",
        "# We freeze the first 6 layers of the model to proceed to train the rest.\n",
        "for layer in VGG19_model.layers[:6]:\n",
        "  layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeT5Ya8qy15l"
      },
      "outputs": [],
      "source": [
        "# We create a new empty model.\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# We add the pre-trained model as if it were a layer.\n",
        "model.add(VGG19_model)\n",
        "\n",
        "# We continue adding more layers that will be trained.\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64, kernel_regularizer=regularizers.l2(0.01), activation = 'relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(num_classes, activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOCsD6A6Q2CB"
      },
      "outputs": [],
      "source": [
        "# We compile the model.\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer= SGD(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxUtWJAeTqSl"
      },
      "outputs": [],
      "source": [
        "# This callback is used to save the model during training.\n",
        "# You can specify various parameters, such as the file name to\n",
        "# save the model to, whether to save only the best model or all models.\n",
        "checkpointer = ModelCheckpoint(filepath='model', verbose=1, save_best_only=True,\n",
        "                               monitor = 'val_acc', mode = 'max')\n",
        "\n",
        "# This callback is used to dynamically adjust the learning rate\n",
        "# during training. Reducing the learning rate can help prevent\n",
        "# the model from getting stuck in a local minimum during training.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=3, min_lr=0.000001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6ikkpdVRoJl",
        "outputId": "c1189c23-b39c-4f16-b1eb-4dc7c596821d"
      },
      "outputs": [],
      "source": [
        "# We train the model.\n",
        "history= model.fit(train_generator,\n",
        "                   validation_data = valid_generator,\n",
        "                   callbacks = [reduce_lr, checkpointer],\n",
        "                   epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB_wZwVjU5a8"
      },
      "outputs": [],
      "source": [
        "# Save trained model.\n",
        "model.save('model_sign_language.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSoZJufayWRq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "Qvu8W0S3SQrC",
        "outputId": "08586bf3-be5c-4a6f-bdda-fc7a472900c5"
      },
      "outputs": [],
      "source": [
        "# Accuracy plot\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['train','test'])\n",
        "plt.show()\n",
        "\n",
        "# Loss plot\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['val_loss','loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "oLKecrGqMiTe",
        "outputId": "a31a56e1-7b06-44ae-ce13-36dc926e56d8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from google.colab import files\n",
        "\n",
        "# Uncomment code if you want to upload image from your PC\n",
        "#uploaded=files.upload()\n",
        "#file=list(uploaded.keys())[0]\n",
        "#path='/content/' + file\n",
        "\n",
        "# Path to one of the example images, ranging from example_0 to example_9\n",
        "path = '/content/Sign-Language-Digits-Dataset/Examples/example_3.JPG'\n",
        "\n",
        "# Load the image and resize it to (28*k, 28*k) as expected by the model\n",
        "img = image.load_img(path, target_size=(28*k, 28*k))\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "img_array = image.img_to_array(img)\n",
        "\n",
        "# Add an additional dimension to represent the batch\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Preprocess the image (normalize, etc.)\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "# Make the prediction\n",
        "predicted_batch = model.predict(img_array)\n",
        "\n",
        "# Get the predicted classes\n",
        "predicted_class = np.argmax(predicted_batch)\n",
        "\n",
        "# Get the name of the predicted class\n",
        "predicted_label = clases[predicted_class]\n",
        "\n",
        "# Show the image with the prediction\n",
        "plt.imshow(img)\n",
        "plt.title('Predicted Class: ' + predicted_label)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
